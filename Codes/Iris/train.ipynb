{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "noPaj0FOvepY",
        "colab_type": "code",
        "outputId": "513a0811-f6d4-48f0-9072-6cdccaf07481",
        "colab": {}
      },
      "source": [
        "'''Training and saving neural network to classify Iris'''\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os,csv,math,sys, joblib\n",
        "import numpy as np\n",
        "import sklearn.model_selection, sklearn.preprocessing\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "class neural_network(nn.Module):\n",
        "    def __init__(self, num_input, num_hidden,num_output):\n",
        "        super(neural_network, self).__init__()\n",
        "        self.hidden = nn.Linear(num_input,num_hidden)\n",
        "        self.out = nn.Linear(num_hidden,num_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden(x))\n",
        "        return self.out(x)\n",
        "\n",
        "# Make results reproducible\n",
        "seed = 1\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Loading the dataset\n",
        "dataset = pd.read_csv('Iris_Dataset.csv')\n",
        "dataset = pd.get_dummies(dataset, columns=['Species']) # One Hot Encoding\n",
        "values = list(dataset.columns.values)\n",
        "\n",
        "y = dataset[values[-3:]]\n",
        "y = np.array(y, dtype='float32')\n",
        "X = dataset[values[:-3]]\n",
        "X = np.array(X, dtype='float32')\n",
        "\n",
        "# Shuffle Data\n",
        "indices = np.random.choice(len(X), len(X), replace=False)\n",
        "X_values = X[indices]\n",
        "y_values = y[indices]\n",
        "\n",
        "# Creating a Train and a Test Dataset\n",
        "test_size = 30\n",
        "X_test = X_values[-test_size:]\n",
        "X_train = X_values[:-test_size]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "y_test = y_values[-test_size:]\n",
        "\n",
        "\n",
        "y_train = y_values[:-test_size]\n",
        "\n",
        "# Interval / Epochs\n",
        "interval = 50\n",
        "epoch = 500\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss(weight=None, size_average=True, ignore_index=-100, reduce=True)\n",
        "\n",
        "# Input neurons : 4\n",
        "# Hidden neurons : 8\n",
        "# Output neurons : 3\n",
        "hidden_layer_nodes = 8\n",
        "model = neural_network(4,hidden_layer_nodes,3)\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                {'params': model.parameters()},\n",
        "            ], lr = 0.0003, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "for epoch in range(0,epoch): \n",
        "    loss_val = 0\n",
        "    for input_data in enumerate(X_train):\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        input_neurons_data = autograd.Variable(torch.FloatTensor(input_data[1]))\n",
        "\n",
        "\n",
        "        one_hot_target = y_train[input_data[0]]\n",
        "        target_class_index = int(np.where(one_hot_target == 1.0)[0])\n",
        "\n",
        "        target_class = autograd.Variable(torch.LongTensor([target_class_index]))\n",
        "        output = model(input_neurons_data)\n",
        "\n",
        "        loss = loss_func(output.view(1,-1),target_class)\n",
        "        loss_val += loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch%interval == 0:\n",
        "        print (loss_val.item()/len(X_train))\n",
        "\n",
        "torch.save(model, \"iris_double_model_8\")\n",
        "\n",
        "error = 0.0\n",
        "for i in range(0,len(X_test)):\n",
        "    predicted = np.argmax(np.array(model(autograd.Variable(torch.FloatTensor(X_test[i]))).data))\n",
        "    actual = int(np.where(y_test[i] == 1.0)[0])\n",
        "    if predicted != actual:\n",
        "        error += 1.0\n",
        "\n",
        "print (error/len(X_test))\n",
        "\n",
        "error = 0.0\n",
        "for i in range(0,len(X_train)):\n",
        "    predicted = np.argmax(np.array(model(autograd.Variable(torch.FloatTensor(X_train[i]))).data))\n",
        "    actual = int(np.where(y_train[i] == 1.0)[0])\n",
        "    if predicted != actual:\n",
        "        error += 1.0\n",
        "\n",
        "print (error/len(X_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.108814493815104\n",
            "0.48065096537272134\n",
            "0.3008557319641113\n",
            "0.20329108238220214\n",
            "0.1418273131052653\n",
            "0.10645585060119629\n",
            "0.08524239857991536\n",
            "0.07196439107259114\n",
            "0.06318517923355102\n",
            "0.057092559337615964\n",
            "0.1\n",
            "0.008333333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type neural_network. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA49H5Xxvep1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}